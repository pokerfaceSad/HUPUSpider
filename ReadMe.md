# HUPUSpider
虎扑步行街数据采集
# Data Analysis

------------------------------------------------------------------------
## Data Pre-processing
------------------------------------------------------------------------
1. 从**Client端**获取每天的帖子，将帖子的**crawl_time**作为标识来进行抓取，同时在查询条件中设置：

  1. 帖子的回复数需要大于0
  2. 帖子的被点亮的回复数需要大于0
  3. 帖子的浏览量需要大于0

  因此，初步的筛选相当于只对帖子的crawl_time进行了处理，只要该帖子在当天的某个时刻被Spider抓取到**一次（及以上）**，就把该帖子认为是当天的帖子;

2. 查询每天的帖子的同时，获取到当天的满足条件的帖子的数量总数，取间断的几天为<br/>例，可以估计每天产生的帖子数。对当天的帖子进行查询后，按照帖子的**点亮数**，**回复数**，**浏览量**进行**降序**排列（点亮数相同比较回复数，回复数相同比较浏览量），并返回<br/>总帖子的**Rank**前5%的帖子；

3. 如果需要查询连续的几天的帖子进行分析，任务可以分解为更改帖子的查询条件，<br/>依次获取到需要查询的帖子并存储。

## Data Post-processing
------------------------------------------------------------------------
1. 在对查询后的帖子进行查询和排序之后，得到了未经过处理的**Str**格式的数据，需要考虑<br/>到帖子中可能有一些无意义的字符（**针对于数据进行**），需要对每一个帖子的源数据<br/>进行**正则匹配后替换**，这里选用的文本有：包含**post_cotent**（帖子标题的详述），<br/>以及帖子的**被点亮的评论**拼接而成的字符串；

2. 对每个帖子进行文本的提取过后，采用了中文语言处理工具**[jieba分词][1]**中的关键词提取算<br/>法**TF-IDF**进行帖子的关键词抽取，参数的设置具体见程序源码，为了使处理的效果更好<br/>，添加了**停用词表**和**大概率成词表**进行修正；

3. 每个热帖有几个关键词，每个帖子的**关键词的权重**按照计算公式：0.4995\*帖子的点亮数\+0.4995\*帖子的被点亮的评论的点亮数\+0.001\*，按照关键词和权重的对应关系生成<br/>词典

## Visualize Result
-------------------------------------------------------------------------
1. 利用在数据处理过程中生成的词典，使用第三方开源库**[WordCloud][2]**进行词云的生成<br/>,参数的设置和调整主要依据词典的大小进行设置

2. 生成的词云的结果已附在提交的文件中



  [1]: https://github.com/fxsjy/jieba    
  [2]: http://amueller.github.io/word_cloud/index.html
