# HUPUSpider
虎扑步行街数据采集
# Data Analysis

----------------------------------------------------------------------------------------
## Data Pre-processing
----------------------------------------------------------------------------------------
1. 从**Client端**获取每天的帖子，将帖子的**crawl_time**作为标识来进行抓取，同时在查询条件中设置：

  1. 帖子的回复数需要大于0
  2. 帖子的被点亮的回复数需要大于0
  3. 帖子的浏览量需要大于0

  因此，初步的筛选相当于只对帖子的crawl_time进行了处理，只要该帖子在当天的某个时刻被Spider抓取<br/>
  到**一次（及以上）**，就把该帖子认为是当天的帖子;

2. 查询每天的帖子的同时，获取到当天的满足条件的帖子的数量总数，取间断的几天为例，可以估计每天<br/>
产生的帖子数。对当天的帖子进行查询后，按照帖子的**点亮数**，**回复数**，**浏览量**进行**降序**排列（点亮<br/>数相同比较回复数，回复数相同的较浏览量），并返回总帖子的**Rank**前5%的帖子；

3. 如果需要查询连续的几天的帖子进行分析，任务可以分解为更改帖子的查询条件，依次获取到需要查询的帖<br/>子
并存储。

## Data Post-processing
--------------------------------------------------------------------------------
1. 在对查询后的帖子进行查询和排序之后，得到了未经过处理的**Str**格式的数据，需要考虑到帖子中可能有<br/>
一些无意义的字符（**针对于数据进行**），需要对每一个帖子的源数据进行**正则匹配后替换**，这里选用的文本有：<br/>
包含**post_cotent**（帖子标题的详述），以及帖子的**被点亮的评论**拼接而成的字符串；

2. 对每个帖子进行文本的提取过后，采用了中文语言处理工具[jieba分词][1]中的关键词提取算法**TF-IDF**进行<br/>
帖子的关键词抽取，参数的设置具体见程序源码，为了使处理的效果更好，添加了**停用词表**和**大概率成词表**<br/>进行修正；

3. 每个热帖有几个关键词，每个帖子的**关键词的权重**按照计算公式：0.4995\*帖子的点亮数\+0.4995\*帖子<br/>
的被点亮的评论的平均点亮数\+0.001\*帖子的回复数，按照关键词和权重的对应关系生成词典

## Visualize Result
--------------------------------------------------------------------------------
1. 利用在数据处理过程中生成的词典，使用第三方开源库[WordCloud][2]进行词云的生成,参数的设置和调整主要<br/>
依据词典的大小进行设置

2. 生成的词云的结果已附在提交的文件中

# To Be Continued


  [1]: https://github.com/fxsjy/jieba    
  [2]: http://amueller.github.io/word_cloud/index.html
